{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-4  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('https://web.stanford.edu/~hastie/ElemStatLearn//datasets/spam.data'), sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renamed the columns as col-1, col-1, ..., col-n\n",
    "d = {}\n",
    "for x in df.columns:\n",
    "    if x == 57:\n",
    "        d[x] = 'label'\n",
    "    else:\n",
    "        d[x] = 'col-'+str(x) \n",
    "\n",
    "df1 = df.rename(columns = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col-0</th>\n",
       "      <th>col-1</th>\n",
       "      <th>col-2</th>\n",
       "      <th>col-3</th>\n",
       "      <th>col-4</th>\n",
       "      <th>col-5</th>\n",
       "      <th>col-6</th>\n",
       "      <th>col-7</th>\n",
       "      <th>col-8</th>\n",
       "      <th>col-9</th>\n",
       "      <th>...</th>\n",
       "      <th>col-48</th>\n",
       "      <th>col-49</th>\n",
       "      <th>col-50</th>\n",
       "      <th>col-51</th>\n",
       "      <th>col-52</th>\n",
       "      <th>col-53</th>\n",
       "      <th>col-54</th>\n",
       "      <th>col-55</th>\n",
       "      <th>col-56</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   col-0  col-1  col-2  col-3  col-4  col-5  col-6  col-7  col-8  col-9  ...  \\\n",
       "0   0.00   0.64   0.64    0.0   0.32   0.00   0.00   0.00   0.00   0.00  ...   \n",
       "1   0.21   0.28   0.50    0.0   0.14   0.28   0.21   0.07   0.00   0.94  ...   \n",
       "2   0.06   0.00   0.71    0.0   1.23   0.19   0.19   0.12   0.64   0.25  ...   \n",
       "3   0.00   0.00   0.00    0.0   0.63   0.00   0.31   0.63   0.31   0.63  ...   \n",
       "4   0.00   0.00   0.00    0.0   0.63   0.00   0.31   0.63   0.31   0.63  ...   \n",
       "\n",
       "   col-48  col-49  col-50  col-51  col-52  col-53  col-54  col-55  col-56  \\\n",
       "0    0.00   0.000     0.0   0.778   0.000   0.000   3.756      61     278   \n",
       "1    0.00   0.132     0.0   0.372   0.180   0.048   5.114     101    1028   \n",
       "2    0.01   0.143     0.0   0.276   0.184   0.010   9.821     485    2259   \n",
       "3    0.00   0.137     0.0   0.137   0.000   0.000   3.537      40     191   \n",
       "4    0.00   0.135     0.0   0.135   0.000   0.000   3.537      40     191   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-a\n",
    "=> Writing functions to implement Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting dataset as 70% of whole for traing and the rest 30% of data for testing\n",
    "def split_data(df, size):\n",
    "    \n",
    "    if isinstance(size, float):\n",
    "        size = round(size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=size)\n",
    "    test_df = df.loc[test_indices] \n",
    "    train_df = df.drop(test_indices) \n",
    "    return train_df, test_df  # train_df_label, train_df_label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Distinguishing continuous & categorical features in dataset\n",
    "def feature_type(df):\n",
    "    feature_types = []\n",
    "    unique_thresh = 15\n",
    "    for f in df.columns:\n",
    "        if f != \"label\":\n",
    "            unique_values = df[f].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= unique_thresh):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best split among all potential splits\n",
    "def best_split(df1, potential_splits):\n",
    "    \n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            df1_below, df1_above = split_df1(df1, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = parent_entropy(df1_below, df1_above)\n",
    "            \n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "\n",
    "# Function to split a node into children\n",
    "def split_df1(df1, split_column, split_value):\n",
    "    \n",
    "    split_column_values = df1[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        df1_below = df1[split_column_values <= split_value]\n",
    "        df1_above = df1[split_column_values >  split_value] \n",
    "    else:\n",
    "        df1_below = df1[split_column_values == split_value]\n",
    "        df1_above = df1[split_column_values != split_value]\n",
    "    \n",
    "    return df1_below, df1_above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#   Function to return classification of data \n",
    "def data_classify(df1):\n",
    "    \n",
    "    label_column = df1[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification\n",
    "\n",
    "\n",
    "# Function to determine all potential splits\n",
    "def potential_splits(df1, random_subspace):\n",
    "    \n",
    "    potential_splits_d = {}\n",
    "    _, n_columns = df1.shape\n",
    "    column_indices = list(range(n_columns - 1))   \n",
    "    \n",
    "    if random_subspace and random_subspace <= len(column_indices):\n",
    "        column_indices = random.sample(population=column_indices, k=random_subspace) # Select k columns that can be potential split\n",
    "    \n",
    "    for column_index in column_indices:          \n",
    "        values = df1[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        potential_splits_d[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits_d\n",
    "\n",
    "\n",
    "# Function to find purity of node\n",
    "def purity(df1):\n",
    "    \n",
    "    label_column = df1[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to find entropy of the node\n",
    "def entropy(df1):\n",
    "    \n",
    "    label_column = df1[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Function to find entropy of a parent node\n",
    "def parent_entropy(df1_below, df1_above):\n",
    "    \n",
    "    n = len(df1_below) + len(df1_above)\n",
    "    p_df1_below = len(df1_below) / n\n",
    "    p_df1_above = len(df1_above) / n\n",
    "\n",
    "    overall_entropy =  (p_df1_below * entropy(df1_below) \n",
    "                      + p_df1_above * entropy(df1_above))\n",
    "    \n",
    "    return overall_entropy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Algorithm function\n",
    "def build_decision_tree(df, counter=0, min_samples=2, max_depth=5, random_subspace=None):\n",
    "    \n",
    "    # This code will be executed for the first level of decision tree\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        FEATURE_TYPES = feature_type(df)\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        df1 = df.values\n",
    "    else:\n",
    "        df1 = df           \n",
    "    \n",
    "    # Termination conditions\n",
    "    if (purity(df1)) or (len(df1) < min_samples) or (counter == max_depth):\n",
    "        classification = data_classify(df1)\n",
    "        \n",
    "        return classification\n",
    "    \n",
    "    # Code executed for building levels of decision tree\n",
    "    else:    \n",
    "        counter += 1\n",
    " \n",
    "        potential_splits_poss = potential_splits(df1, random_subspace)\n",
    "        split_column, split_value = best_split(df1, potential_splits_poss)\n",
    "        df1_below, df1_above = split_df1(df1, split_column, split_value)\n",
    "        \n",
    "        # Empty dataframe\n",
    "        if len(df1_below) == 0 or len(df1_above) == 0:\n",
    "            classification = data_classify(df1)\n",
    "            return classification\n",
    "        \n",
    "        # Find the question on each node of tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            que = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        # feature is categorical\n",
    "        else:\n",
    "            que = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        sub_tree = {que: []}\n",
    "        \n",
    "        # find answers \n",
    "        yes_answer = build_decision_tree(df1_below, counter, min_samples, max_depth, random_subspace)\n",
    "        no_answer = build_decision_tree(df1_above, counter, min_samples, max_depth, random_subspace)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[que].append(yes_answer)\n",
    "            sub_tree[que].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find classification of an instance\n",
    "def predict_example(example, tree):\n",
    "    que = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = que.split(\" \")\n",
    "\n",
    "    # ask que\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[que][0]\n",
    "        else:\n",
    "            answer = tree[que][1]\n",
    "    \n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[que][0]\n",
    "        else:\n",
    "            answer = tree[que][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "    \n",
    "# All examples of the test df1\n",
    "def decision_tree_predictions(test_df, tree):\n",
    "    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n",
    "    return predictions\n",
    "\n",
    "def oob_score(pred, y_test):\n",
    "    mis_label = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != y_test[i]:\n",
    "            mis_label += 1\n",
    "    return mis_label / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, test_df = split_data(df1, size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to bootstrap dataframes\n",
    "def bootstrapping(train_df, n_bootstrap):\n",
    "    bootstrap_indices = np.random.randint(low=0, high=len(train_df), size=n_bootstrap)\n",
    "    df_bootstrapped = train_df.iloc[bootstrap_indices]\n",
    "    df_oob = train_df.iloc[~bootstrap_indices]    \n",
    "    return df_bootstrapped, df_oob\n",
    "\n",
    "# Function to build Random forest\n",
    "def build_random_forest(train_df, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "    forest = []\n",
    "    OOB_data = []\n",
    "    OOB_score = []\n",
    "    for i in range(n_trees):\n",
    "        df_bootstrapped, df_oob = bootstrapping(train_df, n_bootstrap)\n",
    "        tree = build_decision_tree(df_bootstrapped, max_depth=dt_max_depth, random_subspace=n_features)\n",
    "        forest.append(tree)\n",
    "        oob_pred = decision_tree_predictions(df_oob, tree)\n",
    "        oob_predicted = [int(i) for i in oob_pred]\n",
    "        score = oob_score(oob_predicted, df_oob.label.values)        \n",
    "        OOB_score.append(score)\n",
    "    \n",
    "    return forest, np.mean(OOB_score)\n",
    "\n",
    "def random_forest_predictions(test_df, forest):\n",
    "    df_predictions = {}\n",
    "    oob_pred_list = []\n",
    "    for i in range(len(forest)):\n",
    "        column_name = \"tree_{}\".format(i)\n",
    "        predictions = decision_tree_predictions(test_df, tree=forest[i])\n",
    "        df_predictions[column_name] = predictions\n",
    "\n",
    "    df_predictions = pd.DataFrame(df_predictions)\n",
    "    random_forest_predictions = df_predictions.mode(axis=1)[0]\n",
    "    \n",
    "    return random_forest_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-a Compare your results in terms of accuracy and time taken with Scikitlearn’s built-in random forest classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train my random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest, Oob_estimate = build_random_forest(train_df, n_trees=4, n_bootstrap=train_df.shape[0], n_features=30, dt_max_depth=4)\n",
    "predictions = random_forest_predictions(test_df, forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine accuracy, sensitivity and specificity\n",
    "def display_performance(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy = {}\".format(acc))\n",
    "    cm1 = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print(\"Sensitivity =\", sensitivity1)\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.868840579710145\n",
      "Sensitivity = 0.9409638554216867\n",
      "Specificity :  0.76\n"
     ]
    }
   ],
   "source": [
    "display_performance(test_df.label, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a Inbuilt random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_df.iloc[:,:-1], train_df.iloc[:,-1:]\n",
    "x_test, y_test =  test_df.iloc[:,:-1], test_df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 277 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "clf.fit(x_train, y_train.label)\n",
    "pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9144927536231884\n",
      "Sensitivity = 0.9674698795180723\n",
      "Specificity :  0.8345454545454546\n"
     ]
    }
   ],
   "source": [
    "display_performance(test_df.label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-b Explore the sensitivity of Random Forests to the parameter m (the number of features used for best split.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features to choose  10\n",
      "Accuracy = 0.8731884057971014\n",
      "Sensitivity = 0.9650602409638555\n",
      "Specificity :  0.7345454545454545\n",
      "---------\n",
      "number of features to choose  20\n",
      "Accuracy = 0.8753623188405797\n",
      "Sensitivity = 0.9626506024096385\n",
      "Specificity :  0.7436363636363637\n",
      "---------\n",
      "number of features to choose  30\n",
      "Accuracy = 0.8956521739130435\n",
      "Sensitivity = 0.9698795180722891\n",
      "Specificity :  0.7836363636363637\n",
      "---------\n",
      "number of features to choose  40\n",
      "Accuracy = 0.8985507246376812\n",
      "Sensitivity = 0.9554216867469879\n",
      "Specificity :  0.8127272727272727\n",
      "---------\n",
      "number of features to choose  50\n",
      "Accuracy = 0.8891304347826087\n",
      "Sensitivity = 0.9602409638554217\n",
      "Specificity :  0.7818181818181819\n",
      "---------\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = [10,20,30,40,50]\n",
    "for n_feat in n_features:\n",
    "    print(\"number of features to choose \", n_feat)\n",
    "    ## model fitting \n",
    "    forest, oob_estimate = build_random_forest(train_df, n_trees=4, n_bootstrap=800, n_features=30, dt_max_depth=4)\n",
    "    ## model predictions\n",
    "    predictions = random_forest_predictions(test_df, forest)\n",
    "    ## model performance check\n",
    "    display_performance(test_df.label, predictions)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-c Plot the OOB (out-of-bag) error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obb estimates for number of features 5 is 0.14995343061161132\n",
      "Obb estimates for number of features 10 is 0.13443030114871157\n",
      "Obb estimates for number of features 15 is 0.133436820863086\n",
      "Obb estimates for number of features 20 is 0.10959329400807202\n",
      "Obb estimates for number of features 30 is 0.1063644830797889\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = [5,10,15, 20, 30]\n",
    "score = []\n",
    "for e_feat in n_features:\n",
    "    #print(\"number of trees to choose \", estimator)\n",
    "    forest, Obb_estimate = build_random_forest(train_df, n_trees=5, n_bootstrap=train_df.shape[0], n_features=e_feat, dt_max_depth=4)\n",
    "    score.append(Obb_estimate)\n",
    "    print(\"Obb estimates for number of features {} is {}\".format(e_feat, Obb_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2141c569c40>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_features, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
